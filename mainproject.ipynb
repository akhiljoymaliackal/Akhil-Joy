{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mainproject.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akhiljoymaliackal/Akhil-Joy/blob/master/mainproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p3Hi4-Qqztm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrxNcWONry95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqHW0AKer6mr",
        "colab_type": "code",
        "outputId": "6163d7d3-4a39-4e46-e725-2a217193c933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt2WocxUv9bR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH_FnyYqsG4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_width=64\n",
        "input_height=64\n",
        "local_input_width=32\n",
        "local_input_height=32\n",
        "input_channel=3\n",
        "input_dim=100\n",
        "continue_training=False\n",
        "batch_size=64\n",
        "train_size=400\n",
        "Tc=100\n",
        "Td=1\n",
        "learning_rate=0.001\n",
        "momentum=0.5\n",
        "alpha=0.5\n",
        "margin=5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Pd-HEKPsO0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoints_path='/content/gdrive/My Drive/Project/checkpoint'\n",
        "graph_path='/content/gdrive/My Drive/Project/graph'\n",
        "images_path='/content/gdrive/My Drive/Project/image'\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juyligoDsXh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_norm(input, name=\"batch_norm\"):\n",
        "\twith tf.variable_scope(name) as scope:\n",
        "\t\tinput = tf.identity(input)\n",
        "\t\tchannels = input.get_shape()[3]\n",
        "\n",
        "\t\toffset = tf.get_variable(\"offset\", [channels], dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n",
        "\t\tscale = tf.get_variable(\"scale\", [channels], dtype=tf.float32, initializer=tf.random_normal_initializer(1.0, 0.02))\n",
        "\n",
        "\t\tmean, variance = tf.nn.moments(input, axes=[0,1,2], keep_dims=False)\n",
        "\n",
        "\t\tnormalized_batch = tf.nn.batch_normalization(input, mean, variance, offset, scale, variance_epsilon=1e-5)\n",
        "\n",
        "\t\treturn normalized_batch\n",
        "\n",
        "def linear(input, output_size, name=\"linear\"):\n",
        "\tshape = input.get_shape().as_list()\n",
        "\n",
        "\twith tf.variable_scope(name) as scope:\n",
        "\t\tmatrix = tf.get_variable(\"W\", [shape[1], output_size], tf.float32, tf.random_normal_initializer(stddev=0.02))\n",
        "\t\tbias = tf.get_variable(\"bias\", [output_size], initializer=tf.constant_initializer(0.0))\n",
        "\n",
        "\t\treturn tf.matmul(input, matrix) + bias\n",
        "\n",
        "def conv2d(input, out_filter, padding, kernel=5, stride=2, name=\"conv2d\"):\n",
        "\tinput_shape = input.get_shape().as_list()\n",
        "\twith tf.variable_scope(name) as scope:\n",
        "\t\tw = tf.get_variable(\"w\", [kernel, kernel, input_shape[-1], out_filter], initializer=tf.random_normal_initializer(stddev=0.02))\n",
        "\t\tb = tf.get_variable(\"b\", [out_filter], initializer=tf.constant_initializer(0.0))\n",
        "\t\tconv = tf.nn.conv2d(input, w,strides=[1, stride, stride, 1],padding=padding)\n",
        "\n",
        "\t\tconv = tf.reshape(tf.nn.bias_add(conv, b), conv.get_shape())\n",
        "\n",
        "\t\treturn conv\n",
        "\n",
        "def deconv2d(input, out_shape, name=\"deconv2d\"):\n",
        "\tinput_shape = input.get_shape().as_list()\n",
        "\twith tf.variable_scope(name) as scope:\n",
        "\t\tw = tf.get_variable(\"w\", [4, 4, out_shape[-1], input_shape[-1]], initializer=tf.random_normal_initializer(stddev=0.02))\n",
        "\t\tb = tf.get_variable(\"b\", [out_shape[-1]], initializer=tf.constant_initializer(0.0))\n",
        "\t\tdeconv = tf.nn.conv2d_transpose(input, w,output_shape=out_shape,strides=[1, 2, 2, 1],padding=\"SAME\")\n",
        "\t\tdeconv = tf.reshape(tf.nn.bias_add(deconv, b), deconv.get_shape())\n",
        "\n",
        "\t\treturn deconv\n",
        "def dilate_conv2d(input, out_shape, rate, name=\"dilate_conv2d\"):\n",
        "\tinput_shape = input.get_shape().as_list()\n",
        "\twith tf.variable_scope(name) as scope:\n",
        "\t\tw = tf.get_variable(\"w\", [3, 3, input_shape[-1], out_shape[-1]])\n",
        "\t\tb = tf.get_variable(\"b\", [out_shape[-1]], initializer=tf.constant_initializer(0.0))\n",
        "\t\tdilate_conv = tf.nn.atrous_conv2d(input,w,rate=rate,padding=\"SAME\")\n",
        "\t\tdilate_conv = tf.reshape(tf.nn.bias_add(dilate_conv, b), dilate_conv.get_shape())\n",
        "\t\treturn dilate_conv\n",
        "\n",
        "        #return dilate_conv\n",
        "\n",
        "\n",
        "# In[28]:\n",
        "\n",
        "\n",
        "#ops.py\n",
        "def block_patch(input, margin=5):\n",
        "\tshape = input.get_shape().as_list()\n",
        "\n",
        "\t#create patch in random size\n",
        "\tpad_size = tf.random_uniform([2], minval=15, maxval=25, dtype=tf.int32)\n",
        "\tpatch = tf.zeros([pad_size[0], pad_size[1], shape[-1]], dtype=tf.float32)\n",
        "\n",
        "\th_ = tf.random_uniform([1], minval=margin, maxval=shape[0]-pad_size[0]-margin, dtype=tf.int32)[0]\n",
        "\tw_ = tf.random_uniform([1], minval=margin, maxval=shape[1]-pad_size[1]-margin, dtype=tf.int32)[0]\n",
        "\n",
        "\tpadding = [[h_, shape[0]-h_-pad_size[0]], [w_, shape[1]-w_-pad_size[1]], [0, 0]]\n",
        "\tpadded = tf.pad(patch, padding, \"CONSTANT\", constant_values=1)\n",
        "\n",
        "\tcoord = h_, w_\n",
        "\n",
        "\tres = tf.multiply(input, padded)\n",
        "\n",
        "\treturn res, padded, coord, pad_size\n",
        "\n",
        "def load_train_data():\n",
        "\tpaths = '/content/gdrive/My Drive/Project/dataset/*.jpg'\n",
        "\tdata_count = len(glob(paths))\n",
        "\n",
        "\tfilename_queue = tf.train.string_input_producer(tf.train.match_filenames_once(paths))\n",
        "\n",
        "\timage_reader = tf.WholeFileReader()\n",
        "\t_, image_file = image_reader.read(filename_queue)\n",
        "\timages = tf.image.decode_jpeg(image_file, channels=3)\n",
        "\n",
        "\timages = tf.image.resize_images(images ,[input_height, input_width])\n",
        "\timages = tf.image.convert_image_dtype(images, dtype=tf.float32) / 127.5 - 1\n",
        "\n",
        "\torig_images = images\n",
        "\timages, mask, coord, pad_size = block_patch(images, margin=margin)\n",
        "\tmask = tf.reshape(mask, [input_height, input_height, 3])\n",
        "\n",
        "\tmask = -(mask - 1)\n",
        "\timages += mask\n",
        "\n",
        "\torig_imgs, perturbed_imgs, mask, coord, pad_size = tf.train.shuffle_batch([orig_images, images, mask, coord, pad_size],batch_size=batch_size,capacity=batch_size*2,min_after_dequeue=batch_size)\n",
        "\treturn orig_imgs, perturbed_imgs, mask, coord, pad_size, data_count\n",
        "\n",
        "\n",
        "# In[29]:\n",
        "\n",
        "\n",
        "class network():\n",
        "    def __init__(self):\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.input_dim = input_dim \n",
        "\n",
        "        self.local_width, self.local_height = local_input_width, local_input_height\n",
        "\n",
        "        self.m = margin\n",
        "\n",
        "        self.alpha = alpha\n",
        "\n",
        "        #prepare training data\n",
        "        self.real_img, self.perturbed_img, self.mask, self.coord, self.pads, self.data_count = load_train_data()\n",
        "        # self.orig_img, self.test_img, self.test_mask, self.test_data_count = load_test_data(args)\n",
        "        \n",
        "        self.single_orig = tf.placeholder(tf.float32, (batch_size, input_height, input_width, 3))\n",
        "        self.single_test = tf.placeholder(tf.float32, (batch_size, input_height, input_width, 3))\n",
        "        self.single_mask = tf.placeholder(tf.float32, (batch_size, input_height, input_width, 3))\n",
        "\n",
        "        self.build_model()\n",
        "        self.build_loss()\n",
        "\n",
        "        #summary\n",
        "        self.recon_loss_sum = tf.summary.scalar(\"recon_loss\", self.recon_loss) \n",
        "        self.d_loss_sum = tf.summary.scalar(\"d_loss\", self.d_loss) \n",
        "        self.loss_all_sum = tf.summary.scalar(\"loss_all\", self.loss_all)\n",
        "        self.input_img_sum = tf.summary.image(\"input_img\", self.perturbed_img, max_outputs=5)\n",
        "        self.real_img_sum = tf.summary.image(\"real_img\", self.real_img, max_outputs=5)\n",
        "        \n",
        "        self.recon_img_sum = tf.summary.image(\"recon_img\", self.recon_img, max_outputs=5)\n",
        "        self.g_local_imgs_sum = tf.summary.image(\"g_local_imgs\", self.g_local_imgs, max_outputs=5)\n",
        "        self.r_local_imgs_sum = tf.summary.image(\"r_local_imgs\", self.r_local_imgs, max_outputs=5)\n",
        "\n",
        "    #structure of the model\n",
        "    def build_model(self):\n",
        "        def rand_crop(img, coord, pads):\n",
        "          cropped = tf.image.resize_images(tf.image.crop_to_bounding_box(img, coord[0]-self.m, coord[1]-self.m, pads[0]+self.m*2, pads[1]+self.m*2), (self.local_height, self.local_width))\n",
        "          return cropped\n",
        "\n",
        "        # uncomment to concatenate mask and masked input image\n",
        "        # self.perturbed_img = tf.concat([self.perturbed_img, self.mask], -1)\n",
        "\n",
        "        self.recon_img, self.g_nets = self.completion_net(self.perturbed_img, name=\"completion_net\")\n",
        "        self.recon_img = (1-self.mask)*self.real_img + self.mask*self.recon_img\n",
        "\n",
        "        self.test_res_imgs, _ = self.completion_net(self.single_test, name=\"completion_net\", reuse=True)\n",
        "        self.test_res_imgs = (1-self.single_mask)*self.single_orig + self.single_mask*self.test_res_imgs\n",
        "\n",
        "        self.r_local_imgs = []\n",
        "        self.g_local_imgs = [] \n",
        "        for idx in range(0,self.real_img.shape[0]):\n",
        "            r_cropped = rand_crop(self.real_img[idx], self.coord[idx], self.pads[idx])\n",
        "            g_cropped = rand_crop(self.recon_img[idx], self.coord[idx], self.pads[idx])\n",
        "            self.r_local_imgs.append(r_cropped)\n",
        "            self.g_local_imgs.append(g_cropped)\n",
        "\n",
        "\n",
        "        self.r_local_imgs = tf.convert_to_tensor(self.r_local_imgs)\n",
        "        self.g_local_imgs = tf.convert_to_tensor(self.g_local_imgs)\n",
        "        \n",
        "        #global discriminator setting\n",
        "        self.local_fake_d_logits, self.local_fake_d_net = self.local_discriminator(self.g_local_imgs, name=\"local_discriminator\")\n",
        "        self.local_real_d_logits, self.local_real_d_net = self.local_discriminator(self.r_local_imgs, name=\"local_discriminator\", reuse=True)\n",
        "\n",
        "        #local discriminator setting\n",
        "        self.global_fake_d_logits, self.global_fake_d_net = self.global_discriminator(self.recon_img, name=\"global_discriminator\")\n",
        "        self.global_real_d_logits, self.global_real_d_net = self.global_discriminator(self.real_img, name=\"global_discriminator\", reuse=True)\n",
        "\n",
        "        self.fake_d_logits = tf.concat([self.local_fake_d_logits, self.global_fake_d_logits], axis=1)\n",
        "        self.real_d_logits = tf.concat([self.local_real_d_logits, self.global_real_d_logits], axis=1)\n",
        "\n",
        "        self.fake_loss = linear(self.fake_d_logits, 1, \"fake_loss\")\n",
        "        self.real_loss = linear(self.real_d_logits, 1, \"real_loss\")\n",
        "\n",
        "        trainable_vars = tf.trainable_variables()\n",
        "        self.c_vars = []\n",
        "        self.d_vars = []\n",
        "        for var in trainable_vars:\n",
        "            if \"completion_net\" in var.name:\n",
        "                self.c_vars.append(var)\n",
        "            else:\n",
        "                self.d_vars.append(var)\n",
        "\n",
        "    #loss function\n",
        "    def build_loss(self):\n",
        "        def calc_loss(logits, label):\n",
        "            if label==1:\n",
        "                y = tf.ones_like(logits)\n",
        "            else:\n",
        "                y = tf.zeros_like(logits)\n",
        "            return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
        "\n",
        "        self.fake_d_loss = calc_loss(self.fake_loss, 0)\n",
        "        self.real_d_loss = calc_loss(self.real_loss, 1)\n",
        "\n",
        "        #loss to train the discriminator\n",
        "        self.d_loss = self.alpha*(self.fake_d_loss + self.real_d_loss)\n",
        "\n",
        "        self.g_loss = calc_loss(self.fake_loss, 1)\n",
        "        \n",
        "        #mse loss in the paper\n",
        "        self.recon_loss = tf.reduce_mean(tf.nn.l2_loss(self.real_img - self.recon_img))\n",
        "        \n",
        "        self.loss_all = self.recon_loss + self.alpha*self.g_loss\n",
        "\n",
        "    # completion network \n",
        "    def completion_net(self, input, name=\"generator\", reuse=False):\n",
        "        input_shape = input.get_shape().as_list()\n",
        "        nets = []\n",
        "        with tf.variable_scope(name, reuse=reuse) as scope:\n",
        "            conv1 = conv2d(input, 64,kernel=5,stride=1,padding=\"SAME\",name=\"conv1\")\n",
        "            conv1 = batch_norm(conv1, name=\"conv_bn1\")\n",
        "            conv1 = tf.nn.relu(conv1)\n",
        "            \n",
        "            conv2 = conv2d(conv1, 128,kernel=3,stride=2,padding=\"SAME\",name=\"conv2\")\n",
        "            conv2 = batch_norm(conv2, name=\"conv_bn2\")\n",
        "            conv2 = tf.nn.relu(conv2)\n",
        "\n",
        "            conv3 = conv2d(conv2, 128,kernel=3,stride=1,padding=\"SAME\",name=\"conv3\")\n",
        "            conv3 = batch_norm(conv3, name=\"conv_bn3\")\n",
        "            conv3 = tf.nn.relu(conv3)\n",
        "\n",
        "            conv4 = conv2d(conv3, 256,kernel=3,stride=2,padding=\"SAME\",name=\"conv4\")\n",
        "            conv4 = batch_norm(conv4, name=\"conv_bn4\")\n",
        "            conv4 = tf.nn.relu(conv4)\n",
        "\n",
        "            conv5 = conv2d(conv4, 256,kernel=3,stride=1,padding=\"SAME\",name=\"conv5\")\n",
        "            conv5 = batch_norm(conv5, name=\"conv_bn5\")\n",
        "            conv5 = tf.nn.relu(conv5)\n",
        "\n",
        "            conv6 = conv2d(conv5, 256,kernel=3,stride=1,padding=\"SAME\",name=\"conv6\")\n",
        "            conv6 = batch_norm(conv5, name=\"conv_bn6\")\n",
        "            conv6 = tf.nn.relu(conv5)\n",
        "\n",
        "            #Dilated conv from here\n",
        "            dilate_conv1 = dilate_conv2d(conv6,[self.batch_size, conv6.get_shape()[1], conv6.get_shape()[2], 256],rate=2,name=\"dilate_conv1\")\n",
        "\n",
        "            dilate_conv2 = dilate_conv2d(dilate_conv1,[self.batch_size, dilate_conv1.get_shape()[1], dilate_conv1.get_shape()[2], 256],rate=4,name=\"dilate_conv2\")\n",
        "\n",
        "            dilate_conv3 = dilate_conv2d(dilate_conv2,[self.batch_size, dilate_conv2.get_shape()[1], dilate_conv2.get_shape()[2], 256],rate=8,name=\"dilate_conv3\")\n",
        "\n",
        "            dilate_conv4 = dilate_conv2d(dilate_conv3,[self.batch_size, dilate_conv3.get_shape()[1], dilate_conv3.get_shape()[2], 256],rate=16,name=\"dilate_conv4\")                                                                                              \n",
        "\n",
        "            #resize back\n",
        "            conv7 = conv2d(dilate_conv4, 256,kernel=3,stride=1,padding=\"SAME\",name=\"conv7\")\n",
        "            conv7 = batch_norm(conv7, name=\"conv_bn7\")\n",
        "            conv7 = tf.nn.relu(conv7)\n",
        "\n",
        "            conv8 = conv2d(conv7, 256,kernel=3,stride=1,padding=\"SAME\",name=\"conv8\")\n",
        "            conv8 = batch_norm(conv8, name=\"conv_bn8\")\n",
        "            conv8 = tf.nn.relu(conv8)\n",
        "\n",
        "            deconv1 = deconv2d(conv8, [self.batch_size, input_shape[1]/2, input_shape[2]/2, 128], name=\"deconv1\")\n",
        "            deconv1 = batch_norm(deconv1, name=\"deconv_bn1\")\n",
        "            deconv1 = tf.nn.relu(deconv1)\n",
        "\n",
        "            conv9 = conv2d(deconv1, 128,kernel=3,stride=1,padding=\"SAME\",name=\"conv9\")\n",
        "            conv9 = batch_norm(conv9, name=\"conv_bn9\")\n",
        "            conv9 = tf.nn.relu(conv9)\n",
        "\n",
        "            deconv2 = deconv2d(conv9, [self.batch_size, input_shape[1], input_shape[2], 64], name=\"deconv2\")\n",
        "            deconv2 = batch_norm(deconv2, name=\"deconv_bn2\")\n",
        "            deconv2 = tf.nn.relu(deconv2)\n",
        "\n",
        "            conv10 = conv2d(deconv2, 32,kernel=3,stride=1,padding=\"SAME\",name=\"conv10\")\n",
        "            conv10 = batch_norm(conv10, name=\"conv_bn10\")\n",
        "            conv10 = tf.nn.relu(conv10)\n",
        "\n",
        "            conv11 = conv2d(conv10, 3,kernel=3,stride=1,padding=\"SAME\",name=\"conv11\")\n",
        "            conv11 = batch_norm(conv11, name=\"conv_bn11\")\n",
        "            conv11 = tf.nn.tanh(conv11)\n",
        "\n",
        "            return conv11, nets\n",
        "\n",
        "    # D network from DCGAN\n",
        "    def local_discriminator(self, input, name=\"local_discriminator\", reuse=False):\n",
        "        nets = []\n",
        "        with tf.variable_scope(name, reuse=reuse) as scope:\n",
        "            conv1 = tf.contrib.layers.conv2d(input, 64, 5, 2,padding=\"VALID\",activation_fn=None,scope=\"conv1\")\n",
        "            conv1 = batch_norm(conv1, name=\"bn1\")\n",
        "            conv1 = tf.nn.relu(conv1)\n",
        "            nets.append(conv1)\n",
        "\n",
        "            conv2 = tf.contrib.layers.conv2d(conv1, 128, 5, 2,padding=\"VALID\",activation_fn=None,scope=\"conv2\")\n",
        "            conv2 = batch_norm(conv2, name=\"bn2\")\n",
        "            conv2 = tf.nn.relu(conv2)\n",
        "            nets.append(conv2)\n",
        "\n",
        "            conv3 = tf.contrib.layers.conv2d(conv2, 256, 5, 2,padding=\"VALID\",activation_fn=None,scope=\"conv3\")\n",
        "            conv3 = batch_norm(conv1, name=\"bn3\")\n",
        "            conv3 = tf.nn.relu(conv3)\n",
        "            nets.append(conv3)\n",
        "\n",
        "            conv4 = tf.contrib.layers.conv2d(conv3, 512, 5, 2,padding=\"VALID\",activation_fn=None,scope=\"conv4\")\n",
        "            conv4 = batch_norm(conv4, name=\"bn4\")                                                                                                                           \n",
        "            conv4 = tf.nn.relu(conv4)\n",
        "            nets.append(conv4)\n",
        "\n",
        "            conv5 = tf.contrib.layers.conv2d(conv4, 512, 5, 2,padding=\"VALID\",activation_fn=None,scope=\"conv5\")\n",
        "            conv5 = batch_norm(conv5, name=\"bn5\")                                                                                                                           \n",
        "            conv5 = tf.nn.relu(conv5)\n",
        "            nets.append(conv5)\n",
        "\n",
        "            flatten = tf.contrib.layers.flatten(conv5)\n",
        "\n",
        "            output = linear(flatten, 1024, name=\"linear\")\n",
        "\n",
        "            return output, nets\n",
        "\n",
        "\n",
        "\n",
        "    def global_discriminator(self, input, name=\"global_discriminator\", reuse=False):\n",
        "        nets = []\n",
        "        with tf.variable_scope(name, reuse=reuse) as scope:\n",
        "            conv1 = tf.contrib.layers.conv2d(input, 64, 5, 2,padding=\"VALID\",activation_fn=None,scope=\"conv1\")\n",
        "            conv1 = batch_norm(conv1, name=\"bn1\")\n",
        "            conv1 = tf.nn.relu(conv1)\n",
        "            nets.append(conv1)\n",
        "\n",
        "            conv2 = tf.contrib.layers.conv2d(conv1, 128, 5, 2,padding=\"VALID\",activation_fn=None,scope=\"conv2\")\n",
        "            conv2 = batch_norm(conv2, name=\"bn2\")\n",
        "            conv2 = tf.nn.relu(conv2)\n",
        "            nets.append(conv2)\n",
        "\n",
        "            conv3 = tf.contrib.layers.conv2d(conv2, 256, 5, 2,padding=\"VALID\",activation_fn=None,scope=\"conv3\")\n",
        "            conv3 = batch_norm(conv1, name=\"bn3\")\n",
        "            conv3 = tf.nn.relu(conv3)\n",
        "            nets.append(conv3)\n",
        "\n",
        "            conv4 = tf.contrib.layers.conv2d(conv3, 512, 5, 2,padding=\"VALID\",activation_fn=None,scope=\"conv4\")\n",
        "            conv4 = batch_norm(conv4, name=\"bn4\")                                                                                                                           \n",
        "            conv4 = tf.nn.relu(conv4)\n",
        "            nets.append(conv4)\n",
        "\n",
        "            conv5 = tf.contrib.layers.conv2d(conv4, 512, 5, 2,padding=\"VALID\",activation_fn=None,scope=\"conv5\")\n",
        "            conv5 = batch_norm(conv5, name=\"bn5\")                                                                                                                           \n",
        "            conv5 = tf.nn.relu(conv5)\n",
        "            nets.append(conv5)\n",
        "\n",
        "            conv6 = tf.contrib.layers.conv2d(conv5, 512, 5, 2,padding=\"VALID\",activation_fn=None,scope=\"conv6\")\n",
        "            conv6 = batch_norm(conv6, name=\"bn6\")                                                                                                                           \n",
        "            conv6 = tf.nn.relu(conv6)\n",
        "            nets.append(conv6)\n",
        "\n",
        "\n",
        "            flatten = tf.contrib.layers.flatten(conv6)\n",
        "\n",
        "            output = linear(flatten, 1024, name=\"linear\")\n",
        "\n",
        "\n",
        "            return output, nets\n",
        "\n",
        "\n",
        "# In[30]:\n",
        "\n",
        "\n",
        "#train\n",
        "def train(sess, model):\n",
        "    #Adam optimizers are used instead of AdaDelta\n",
        "    d_optimizer = tf.train.AdamOptimizer(learning_rate, beta1=momentum, name=\"AdamOptimizer_D\").minimize(model.d_loss, var_list=model.d_vars)\n",
        "    c_optimizer = tf.train.AdamOptimizer(learning_rate, beta1=momentum, name=\"AdamOptimizer_C\").minimize(model.recon_loss, var_list=model.c_vars)\n",
        "\n",
        "    global_optimizer = tf.train.AdamOptimizer(learning_rate, beta1=momentum, name=\"AdamOptimizer_C\").minimize(model.loss_all, var_list=model.c_vars)\n",
        "\n",
        "    epoch = 0\n",
        "    step = 0\n",
        "    global_step = 0\n",
        "\n",
        "    #saver\n",
        "    saver = tf.train.Saver()\n",
        "    if continue_training:\n",
        "        tf.local_variables_initializer().run()\n",
        "        last_ckpt = tf.train.latest_checkpoint(checkpoints_path)\n",
        "        saver.restore(sess, last_ckpt)\n",
        "        ckpt_name = str(last_ckpt)\n",
        "        print(\"Loaded model file from \" + ckpt_name)\n",
        "        epoch = int(ckpt_name.split('-')[-1])\n",
        "    else:\n",
        "        tf.global_variables_initializer().run()\n",
        "        tf.local_variables_initializer().run()\n",
        "\n",
        "\n",
        "\n",
        "    coord = tf.train.Coordinator()\n",
        "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
        "\n",
        "    #summary init\n",
        "    all_summary = tf.summary.merge([model.recon_loss_sum,\n",
        "                                    model.d_loss_sum,\n",
        "                                    model.loss_all_sum,\n",
        "                                    model.input_img_sum,\n",
        "                                    model.real_img_sum,\n",
        "                                    model.recon_img_sum,\n",
        "                                    model.g_local_imgs_sum,\n",
        "                                    model.r_local_imgs_sum])\n",
        "    writer = tf.summary.FileWriter(graph_path, sess.graph)\n",
        "\n",
        "    #training starts here\n",
        "\n",
        "    #first train completion network\n",
        "    while epoch <400:\n",
        "        print(\"hai\")\n",
        "        #Training Stage 1 (Completion Network)\n",
        "        if epoch < Tc:\n",
        "            summary, c_loss, _ = sess.run([all_summary, model.recon_loss, c_optimizer])\n",
        "            writer.add_summary(summary, global_step)\n",
        "            print(\"Epoch [%d] Step [%d] C Loss: [%.4f]\" % (epoch, step, c_loss))\n",
        "        elif epoch < Tc + Td:\n",
        "            #Training Stage 2 (Discriminator Network)\n",
        "            summary, d_loss, _ = sess.run([all_summary, model.d_loss, d_optimizer])\n",
        "            writer.add_summary(summary, global_step)\n",
        "            print (\"Epoch [%d] Step [%d] D Loss: [%.4f]\" % (epoch, step, d_loss))\n",
        "        else:\n",
        "            #Training Stage 3 (Completion Network)\n",
        "            summary, g_loss, _ = sess.run([all_summary, model.loss_all, global_optimizer])\n",
        "            writer.add_summary(summary, global_step)\n",
        "            print (\"Epoch [%d] Step [%d] C Loss: [%.4f]\" % (epoch, step, g_loss))\n",
        "\n",
        "\n",
        "        # Check Test image results every time epoch is finished\n",
        "        if step*batch_size >= model.data_count:\n",
        "            saver.save(sess, checkpoints_path + \"/model\", global_step=epoch)\n",
        "\n",
        "            #res_img = sess.run(model.test_res_imgs)\n",
        "\n",
        "            # save test img result\n",
        "            #img_tile(epoch, args, res_img)\n",
        "            step = 0\n",
        "            epoch += 1\n",
        "\n",
        "        step += 1\n",
        "        global_step += 1\n",
        "       \n",
        "      \n",
        "    \n",
        "     \n",
        "      \n",
        "\n",
        "\n",
        "    coord.request_stop()\n",
        "    coord.join(threads)\n",
        "    sess.close()\n",
        "    print(\"Done.\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    run_config = tf.ConfigProto()\n",
        "    run_config.gpu_options.allow_growth = True\n",
        "\n",
        "    #create graph, images, and checkpoints folder if they don't exist\n",
        "    if not os.path.exists(checkpoints_path):\n",
        "        os.makedirs(checkpoints_path)\n",
        "    if not os.path.exists(graph_path):\n",
        "        os.makedirs(graph_path)\n",
        "    if not os.path.exists(images_path):\n",
        "        os.makedirs(images_path)\n",
        "\n",
        "    with tf.Session(config=run_config) as sess:\n",
        "        model = network()\n",
        "\n",
        "        print('Start Training...')\n",
        "        train(sess, model)\n",
        "\n",
        "main()\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snOrQE--sdH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0brBYm_Zs3uH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sho2hwuws_R1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmKX434otECR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO90fxqJsN7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}